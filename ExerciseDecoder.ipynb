{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d602e75d",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d97e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# disable some of the tf/keras training warnings \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "# suppress untraced functions warning\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f470e2",
   "metadata": {},
   "source": [
    "# 1. Keypoints using MP Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cde117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained pose estimation model from Google Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Supported Mediapipe visualization tools\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from webcam footage\n",
    "    \n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "# Caminho ajustado para o vídeo\n",
    "cap = cv2.VideoCapture('D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir o arquivo de vídeo D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4\")\n",
    "    exit()\n",
    "\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Limitar o número de frames processados para testes (podes ajustar conforme necessário)\n",
    "max_frames = 100  # Processar apenas 100 frames para testes rápidos\n",
    "\n",
    "# Set and test mediapipe model using video file\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, \n",
    "                  static_image_mode=False, model_complexity=1) as pose:\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fim do vídeo ou falha na leitura.\")\n",
    "            break\n",
    "        \n",
    "        # Redimensionar o frame para fornecer dimensões consistentes ao MediaPipe\n",
    "        image = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "        \n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(image, pose)\n",
    "        \n",
    "        # Extract landmarks and keypoints\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            pose_keypoints = np.array([[res.x, res.y, res.z, res.visibility] for res in landmarks]).flatten()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render detections (comentar para desativar a visualização e acelerar)\n",
    "        # draw_landmarks(image, results)               \n",
    "        \n",
    "        # Redimensionar o frame para 1280x720 (comentar para desativar a visualização e acelerar)\n",
    "        # image = cv2.resize(image, (1280, 720))\n",
    "        \n",
    "        # Display frame on screen (comentar para desativar a visualização e acelerar)\n",
    "        # cv2.imshow('OpenCV Feed', image)\n",
    "        # cv2.waitKey(1)  # Adicionado para garantir a visualização\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Exit / break out logic (comentar para desativar a visualização e acelerar)\n",
    "        # if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()  # Comentar para desativar a visualização e acelerar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb32f3",
   "metadata": {},
   "source": [
    "# 2. Extract Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b907e8",
   "metadata": {},
   "source": [
    "# 3. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcaecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\uni\\Exercise_Recognition_AI\\data\n"
     ]
    }
   ],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'data') \n",
    "print(DATA_PATH)\n",
    "\n",
    "# make directory if it does not exist yet\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Actions/exercises that we try to detect (adicionando flexões)\n",
    "actions = np.array(['curl', 'press', 'squat', 'pushup'])  # Adicionamos 'pushup'\n",
    "num_classes = len(actions)\n",
    "\n",
    "# How many videos worth of data\n",
    "no_sequences = 50\n",
    "\n",
    "# Videos are going to be this many frames in length\n",
    "sequence_length = FPS*1\n",
    "\n",
    "# Folder start\n",
    "# Change this to collect more data and not lose previously collected data\n",
    "start_folder = 101\n",
    "\n",
    "# Build folder paths\n",
    "for action in actions:     \n",
    "    for sequence in range(start_folder, no_sequences + start_folder):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))  \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b573",
   "metadata": {},
   "source": [
    "# 4. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors associated with each exercise (e.g., curls are denoted by blue, squats are denoted by orange, etc.)\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (255,0,0)]  # Adicionada uma cor para 'pushup'\n",
    "\n",
    "# Collect Training Data\n",
    "cap = cv2.VideoCapture('D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir o arquivo de vídeo D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4\")\n",
    "    exit()\n",
    "\n",
    "# Aumentar o número de sequências para melhorar o treino\n",
    "no_sequences = 30  # Aumentei de 10 para 30 para mais dados de treino\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, \n",
    "                  static_image_mode=False, model_complexity=1) as pose:\n",
    "    # Loop through actions\n",
    "    for idx, action in enumerate(actions):\n",
    "        # Loop through sequences (i.e., videos)\n",
    "        for sequence in range(start_folder, start_folder + no_sequences):\n",
    "            # Loop through video length (i.e, sequence length)\n",
    "            frame_count = 0\n",
    "            while frame_count < sequence_length:\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Não há mais frames para a ação {action}, sequência {sequence}\")\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia o vídeo para a próxima sequência\n",
    "                    break\n",
    "                \n",
    "                # Redimensionar o frame para fornecer dimensões consistentes ao MediaPipe\n",
    "                image = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "                \n",
    "                # Make detection\n",
    "                image, results = mediapipe_detection(image, pose)\n",
    "\n",
    "                # Extract landmarks\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Export keypoints (sequence + pose landmarks)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                frame_count += 1\n",
    "\n",
    "            # Após coletar os frames necessários, reinicia o vídeo para a próxima sequência\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ec993",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad528c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 30, 132) (200, 4)\n",
      "(180, 30, 132) (180, 4)\n"
     ]
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "# Load and organize recorded training data\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):         \n",
    "            # LSTM input data\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)  \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# Make sure first dimensions of arrays match\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split into training, validation, and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ae03d",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912f3153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">133,632</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m133,632\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m394,240\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m197,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m260\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">750,020</span> (2.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m750,020\u001b[0m (2.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">750,020</span> (2.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m750,020\u001b[0m (2.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - categorical_accuracy: 0.1735 - loss: 3.4499\n",
      "Epoch 1: val_loss improved from inf to 3.20581, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_01-3.21.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 197ms/step - categorical_accuracy: 0.1765 - loss: 3.4439 - val_categorical_accuracy: 0.2667 - val_loss: 3.2058 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - categorical_accuracy: 0.2299 - loss: 3.1249\n",
      "Epoch 2: val_loss improved from 3.20581 to 2.90203, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_02-2.90.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - categorical_accuracy: 0.2333 - loss: 3.1188 - val_categorical_accuracy: 0.1000 - val_loss: 2.9020 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2615 - loss: 2.8384\n",
      "Epoch 3: val_loss improved from 2.90203 to 2.65957, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_03-2.66.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - categorical_accuracy: 0.2661 - loss: 2.8277 - val_categorical_accuracy: 0.1000 - val_loss: 2.6596 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.2824 - loss: 2.5956\n",
      "Epoch 4: val_loss improved from 2.65957 to 2.44671, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_04-2.45.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - categorical_accuracy: 0.2832 - loss: 2.5864 - val_categorical_accuracy: 0.1000 - val_loss: 2.4467 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.2857 - loss: 2.3813\n",
      "Epoch 5: val_loss improved from 2.44671 to 2.26731, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_05-2.27.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - categorical_accuracy: 0.2858 - loss: 2.3780 - val_categorical_accuracy: 0.1000 - val_loss: 2.2673 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2425 - loss: 2.2145\n",
      "Epoch 6: val_loss improved from 2.26731 to 2.11460, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_06-2.11.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - categorical_accuracy: 0.2465 - loss: 2.2111 - val_categorical_accuracy: 0.1000 - val_loss: 2.1146 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.3223 - loss: 2.0626\n",
      "Epoch 7: val_loss improved from 2.11460 to 1.99623, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_07-2.00.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - categorical_accuracy: 0.3158 - loss: 2.0578 - val_categorical_accuracy: 0.1000 - val_loss: 1.9962 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.2980 - loss: 1.9400\n",
      "Epoch 8: val_loss improved from 1.99623 to 1.89402, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_08-1.89.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - categorical_accuracy: 0.2959 - loss: 1.9363 - val_categorical_accuracy: 0.1000 - val_loss: 1.8940 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - categorical_accuracy: 0.2945 - loss: 1.8370\n",
      "Epoch 9: val_loss improved from 1.89402 to 1.79084, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_09-1.79.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - categorical_accuracy: 0.2938 - loss: 1.8357 - val_categorical_accuracy: 0.1000 - val_loss: 1.7908 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.2694 - loss: 1.7599\n",
      "Epoch 10: val_loss improved from 1.79084 to 1.71706, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_10-1.72.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - categorical_accuracy: 0.2710 - loss: 1.7584 - val_categorical_accuracy: 0.1000 - val_loss: 1.7171 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2309 - loss: 1.6927\n",
      "Epoch 11: val_loss improved from 1.71706 to 1.65695, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_11-1.66.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - categorical_accuracy: 0.2410 - loss: 1.6898 - val_categorical_accuracy: 0.1000 - val_loss: 1.6570 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.3206 - loss: 1.6284\n",
      "Epoch 12: val_loss improved from 1.65695 to 1.61409, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_12-1.61.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - categorical_accuracy: 0.3144 - loss: 1.6274 - val_categorical_accuracy: 0.1000 - val_loss: 1.6141 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.2649 - loss: 1.5845\n",
      "Epoch 13: val_loss improved from 1.61409 to 1.57464, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_13-1.57.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 191ms/step - categorical_accuracy: 0.2669 - loss: 1.5838 - val_categorical_accuracy: 0.1000 - val_loss: 1.5746 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2835 - loss: 1.5486\n",
      "Epoch 14: val_loss improved from 1.57464 to 1.54355, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_14-1.54.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - categorical_accuracy: 0.2838 - loss: 1.5479 - val_categorical_accuracy: 0.1000 - val_loss: 1.5436 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - categorical_accuracy: 0.2517 - loss: 1.5200\n",
      "Epoch 15: val_loss improved from 1.54355 to 1.51703, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_15-1.52.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - categorical_accuracy: 0.2548 - loss: 1.5192 - val_categorical_accuracy: 0.1000 - val_loss: 1.5170 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.3156 - loss: 1.4879\n",
      "Epoch 16: val_loss improved from 1.51703 to 1.49959, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_16-1.50.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 166ms/step - categorical_accuracy: 0.3104 - loss: 1.4877 - val_categorical_accuracy: 0.1000 - val_loss: 1.4996 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.3460 - loss: 1.4639\n",
      "Epoch 17: val_loss improved from 1.49959 to 1.48511, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_17-1.49.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 232ms/step - categorical_accuracy: 0.3406 - loss: 1.4642 - val_categorical_accuracy: 0.1000 - val_loss: 1.4851 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2485 - loss: 1.4615\n",
      "Epoch 18: val_loss improved from 1.48511 to 1.47061, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_18-1.47.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 258ms/step - categorical_accuracy: 0.2554 - loss: 1.4596 - val_categorical_accuracy: 0.1000 - val_loss: 1.4706 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - categorical_accuracy: 0.2573 - loss: 1.4479\n",
      "Epoch 19: val_loss improved from 1.47061 to 1.45772, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_19-1.46.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 278ms/step - categorical_accuracy: 0.2626 - loss: 1.4462 - val_categorical_accuracy: 0.1000 - val_loss: 1.4577 - learning_rate: 0.0010\n",
      "Epoch 20/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - categorical_accuracy: 0.2830 - loss: 1.4306\n",
      "Epoch 20: val_loss improved from 1.45772 to 1.45209, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_20-1.45.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 171ms/step - categorical_accuracy: 0.2837 - loss: 1.4300 - val_categorical_accuracy: 0.1000 - val_loss: 1.4521 - learning_rate: 0.0010\n",
      "Epoch 21/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2980 - loss: 1.4190\n",
      "Epoch 21: val_loss improved from 1.45209 to 1.44799, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_21-1.45.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 163ms/step - categorical_accuracy: 0.2959 - loss: 1.4190 - val_categorical_accuracy: 0.1000 - val_loss: 1.4480 - learning_rate: 0.0010\n",
      "Epoch 22/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2752 - loss: 1.4142\n",
      "Epoch 22: val_loss improved from 1.44799 to 1.44403, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_22-1.44.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - categorical_accuracy: 0.2763 - loss: 1.4140 - val_categorical_accuracy: 0.1000 - val_loss: 1.4440 - learning_rate: 0.0010\n",
      "Epoch 23/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.3153 - loss: 1.4040\n",
      "Epoch 23: val_loss improved from 1.44403 to 1.44296, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_23-1.44.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - categorical_accuracy: 0.3127 - loss: 1.4043 - val_categorical_accuracy: 0.1000 - val_loss: 1.4430 - learning_rate: 0.0010\n",
      "Epoch 24/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2758 - loss: 1.4114\n",
      "Epoch 24: val_loss improved from 1.44296 to 1.43009, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_24-1.43.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - categorical_accuracy: 0.2778 - loss: 1.4100 - val_categorical_accuracy: 0.1000 - val_loss: 1.4301 - learning_rate: 0.0010\n",
      "Epoch 25/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2862 - loss: 1.3998\n",
      "Epoch 25: val_loss improved from 1.43009 to 1.42767, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_25-1.43.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - categorical_accuracy: 0.2862 - loss: 1.3997 - val_categorical_accuracy: 0.1000 - val_loss: 1.4277 - learning_rate: 0.0010\n",
      "Epoch 26/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2786 - loss: 1.3974\n",
      "Epoch 26: val_loss improved from 1.42767 to 1.42649, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_26-1.43.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - categorical_accuracy: 0.2793 - loss: 1.3972 - val_categorical_accuracy: 0.1000 - val_loss: 1.4265 - learning_rate: 0.0010\n",
      "Epoch 27/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2286 - loss: 1.4011\n",
      "Epoch 27: val_loss improved from 1.42649 to 1.42314, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_27-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - categorical_accuracy: 0.2391 - loss: 1.3998 - val_categorical_accuracy: 0.1000 - val_loss: 1.4231 - learning_rate: 0.0010\n",
      "Epoch 28/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2968 - loss: 1.3923\n",
      "Epoch 28: val_loss improved from 1.42314 to 1.42296, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_28-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - categorical_accuracy: 0.2958 - loss: 1.3923 - val_categorical_accuracy: 0.1000 - val_loss: 1.4230 - learning_rate: 0.0010\n",
      "Epoch 29/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2840 - loss: 1.3909\n",
      "Epoch 29: val_loss improved from 1.42296 to 1.42121, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_29-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - categorical_accuracy: 0.2845 - loss: 1.3908 - val_categorical_accuracy: 0.1000 - val_loss: 1.4212 - learning_rate: 0.0010\n",
      "Epoch 30/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2862 - loss: 1.3854\n",
      "Epoch 30: val_loss improved from 1.42121 to 1.41942, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_30-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - categorical_accuracy: 0.2863 - loss: 1.3860 - val_categorical_accuracy: 0.1000 - val_loss: 1.4194 - learning_rate: 0.0010\n",
      "Epoch 31/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2842 - loss: 1.3870\n",
      "Epoch 31: val_loss improved from 1.41942 to 1.41881, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_31-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 209ms/step - categorical_accuracy: 0.2845 - loss: 1.3871 - val_categorical_accuracy: 0.1000 - val_loss: 1.4188 - learning_rate: 0.0010\n",
      "Epoch 32/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.2548 - loss: 1.3916\n",
      "Epoch 32: val_loss improved from 1.41881 to 1.41794, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_32-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - categorical_accuracy: 0.2606 - loss: 1.3907 - val_categorical_accuracy: 0.1000 - val_loss: 1.4179 - learning_rate: 0.0010\n",
      "Epoch 33/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2539 - loss: 1.3925\n",
      "Epoch 33: val_loss improved from 1.41794 to 1.41791, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_33-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - categorical_accuracy: 0.2599 - loss: 1.3913 - val_categorical_accuracy: 0.1000 - val_loss: 1.4179 - learning_rate: 0.0010\n",
      "Epoch 34/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2449 - loss: 1.3930\n",
      "Epoch 34: val_loss improved from 1.41791 to 1.41718, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_34-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - categorical_accuracy: 0.2525 - loss: 1.3917 - val_categorical_accuracy: 0.1000 - val_loss: 1.4172 - learning_rate: 0.0010\n",
      "Epoch 35/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2833 - loss: 1.3866\n",
      "Epoch 35: val_loss did not improve from 1.41718\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - categorical_accuracy: 0.2839 - loss: 1.3863 - val_categorical_accuracy: 0.1000 - val_loss: 1.4183 - learning_rate: 0.0010\n",
      "Epoch 36/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2909 - loss: 1.3800\n",
      "Epoch 36: val_loss did not improve from 1.41718\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - categorical_accuracy: 0.2901 - loss: 1.3809 - val_categorical_accuracy: 0.1000 - val_loss: 1.4198 - learning_rate: 0.0010\n",
      "Epoch 37/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2966 - loss: 1.3854\n",
      "Epoch 37: val_loss did not improve from 1.41718\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - categorical_accuracy: 0.2957 - loss: 1.3853 - val_categorical_accuracy: 0.1000 - val_loss: 1.4197 - learning_rate: 0.0010\n",
      "Epoch 38/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.3034 - loss: 1.3822\n",
      "Epoch 38: val_loss did not improve from 1.41718\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - categorical_accuracy: 0.3003 - loss: 1.3825 - val_categorical_accuracy: 0.1000 - val_loss: 1.4188 - learning_rate: 0.0010\n",
      "Epoch 39/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2129 - loss: 1.3948\n",
      "Epoch 39: val_loss improved from 1.41718 to 1.41602, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_39-1.42.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - categorical_accuracy: 0.2263 - loss: 1.3928 - val_categorical_accuracy: 0.1000 - val_loss: 1.4160 - learning_rate: 0.0010\n",
      "Epoch 40/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2948 - loss: 1.3850\n",
      "Epoch 40: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - categorical_accuracy: 0.2941 - loss: 1.3849 - val_categorical_accuracy: 0.1000 - val_loss: 1.4177 - learning_rate: 0.0010\n",
      "Epoch 41/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.2696 - loss: 1.3877\n",
      "Epoch 41: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - categorical_accuracy: 0.2712 - loss: 1.3873 - val_categorical_accuracy: 0.1000 - val_loss: 1.4188 - learning_rate: 0.0010\n",
      "Epoch 42/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.3052 - loss: 1.3805\n",
      "Epoch 42: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - categorical_accuracy: 0.3018 - loss: 1.3810 - val_categorical_accuracy: 0.1000 - val_loss: 1.4199 - learning_rate: 0.0010\n",
      "Epoch 43/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.3158 - loss: 1.3813\n",
      "Epoch 43: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.3105 - loss: 1.3816 - val_categorical_accuracy: 0.1000 - val_loss: 1.4194 - learning_rate: 0.0010\n",
      "Epoch 44/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2980 - loss: 1.3822\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - categorical_accuracy: 0.2960 - loss: 1.3824 - val_categorical_accuracy: 0.1000 - val_loss: 1.4187 - learning_rate: 0.0010\n",
      "Epoch 45/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2780 - loss: 1.3791\n",
      "Epoch 45: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - categorical_accuracy: 0.2788 - loss: 1.3794 - val_categorical_accuracy: 0.1000 - val_loss: 1.4182 - learning_rate: 5.0000e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2705 - loss: 1.3844\n",
      "Epoch 46: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - categorical_accuracy: 0.2735 - loss: 1.3841 - val_categorical_accuracy: 0.1000 - val_loss: 1.4176 - learning_rate: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2685 - loss: 1.3898\n",
      "Epoch 47: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - categorical_accuracy: 0.2718 - loss: 1.3885 - val_categorical_accuracy: 0.1000 - val_loss: 1.4175 - learning_rate: 5.0000e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.3014 - loss: 1.3792\n",
      "Epoch 48: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - categorical_accuracy: 0.3001 - loss: 1.3796 - val_categorical_accuracy: 0.1000 - val_loss: 1.4181 - learning_rate: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.2827 - loss: 1.3791\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.2834 - loss: 1.3797 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.3274 - loss: 1.3762\n",
      "Epoch 50: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - categorical_accuracy: 0.3200 - loss: 1.3773 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 2.5000e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.3157 - loss: 1.3766\n",
      "Epoch 51: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - categorical_accuracy: 0.3131 - loss: 1.3772 - val_categorical_accuracy: 0.1000 - val_loss: 1.4184 - learning_rate: 2.5000e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2802 - loss: 1.3869\n",
      "Epoch 52: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - categorical_accuracy: 0.2814 - loss: 1.3861 - val_categorical_accuracy: 0.1000 - val_loss: 1.4179 - learning_rate: 2.5000e-04\n",
      "Epoch 53/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2727 - loss: 1.3842\n",
      "Epoch 53: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - categorical_accuracy: 0.2753 - loss: 1.3839 - val_categorical_accuracy: 0.1000 - val_loss: 1.4178 - learning_rate: 2.5000e-04\n",
      "Epoch 54/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.3128 - loss: 1.3800\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.3104 - loss: 1.3802 - val_categorical_accuracy: 0.1000 - val_loss: 1.4181 - learning_rate: 2.5000e-04\n",
      "Epoch 55/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.3224 - loss: 1.3767\n",
      "Epoch 55: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - categorical_accuracy: 0.3159 - loss: 1.3778 - val_categorical_accuracy: 0.1000 - val_loss: 1.4183 - learning_rate: 1.2500e-04\n",
      "Epoch 56/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.2684 - loss: 1.3834\n",
      "Epoch 56: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - categorical_accuracy: 0.2717 - loss: 1.3832 - val_categorical_accuracy: 0.1000 - val_loss: 1.4183 - learning_rate: 1.2500e-04\n",
      "Epoch 57/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.3266 - loss: 1.3750\n",
      "Epoch 57: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.3193 - loss: 1.3763 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 1.2500e-04\n",
      "Epoch 58/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2607 - loss: 1.3849\n",
      "Epoch 58: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.2654 - loss: 1.3845 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 1.2500e-04\n",
      "Epoch 59/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2315 - loss: 1.3952\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - categorical_accuracy: 0.2365 - loss: 1.3940 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 1.2500e-04\n",
      "Epoch 60/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.3135 - loss: 1.3742\n",
      "Epoch 60: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.3086 - loss: 1.3757 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 6.2500e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2737 - loss: 1.3882\n",
      "Epoch 61: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.2760 - loss: 1.3871 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 6.2500e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2642 - loss: 1.3867\n",
      "Epoch 62: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - categorical_accuracy: 0.2662 - loss: 1.3863 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 6.2500e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2489 - loss: 1.3876\n",
      "Epoch 63: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.2557 - loss: 1.3866 - val_categorical_accuracy: 0.1000 - val_loss: 1.4184 - learning_rate: 6.2500e-05\n",
      "Epoch 64/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2646 - loss: 1.3824\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - categorical_accuracy: 0.2687 - loss: 1.3824 - val_categorical_accuracy: 0.1000 - val_loss: 1.4184 - learning_rate: 6.2500e-05\n",
      "Epoch 65/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2739 - loss: 1.3846\n",
      "Epoch 65: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - categorical_accuracy: 0.2751 - loss: 1.3844 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 3.1250e-05\n",
      "Epoch 66/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2928 - loss: 1.3786\n",
      "Epoch 66: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - categorical_accuracy: 0.2922 - loss: 1.3790 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 3.1250e-05\n",
      "Epoch 67/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2430 - loss: 1.3895\n",
      "Epoch 67: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - categorical_accuracy: 0.2510 - loss: 1.3882 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 3.1250e-05\n",
      "Epoch 68/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2548 - loss: 1.3876\n",
      "Epoch 68: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.2577 - loss: 1.3872 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 3.1250e-05\n",
      "Epoch 69/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2520 - loss: 1.3873\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.2583 - loss: 1.3864 - val_categorical_accuracy: 0.1000 - val_loss: 1.4185 - learning_rate: 3.1250e-05\n",
      "Epoch 70/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.2898 - loss: 1.3848\n",
      "Epoch 70: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.2892 - loss: 1.3843 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.5625e-05\n",
      "Epoch 71/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.2952 - loss: 1.3822\n",
      "Epoch 71: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - categorical_accuracy: 0.2944 - loss: 1.3822 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.5625e-05\n",
      "Epoch 72/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.2221 - loss: 1.3970\n",
      "Epoch 72: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - categorical_accuracy: 0.2339 - loss: 1.3943 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.5625e-05\n",
      "Epoch 73/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.3248 - loss: 1.3745\n",
      "Epoch 73: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.3179 - loss: 1.3759 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.5625e-05\n",
      "Epoch 74/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.3175 - loss: 1.3803\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - categorical_accuracy: 0.3119 - loss: 1.3806 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.5625e-05\n",
      "Epoch 75/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.2708 - loss: 1.3874\n",
      "Epoch 75: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - categorical_accuracy: 0.2722 - loss: 1.3869 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 76/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2378 - loss: 1.3923\n",
      "Epoch 76: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.2422 - loss: 1.3914 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 77/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2970 - loss: 1.3810\n",
      "Epoch 77: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - categorical_accuracy: 0.2951 - loss: 1.3813 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.3332 - loss: 1.3743\n",
      "Epoch 78: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - categorical_accuracy: 0.3247 - loss: 1.3757 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 79/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.2946 - loss: 1.3802\n",
      "Epoch 79: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - categorical_accuracy: 0.2932 - loss: 1.3806 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 80/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2768 - loss: 1.3889\n",
      "Epoch 80: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - categorical_accuracy: 0.2786 - loss: 1.3877 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 81/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.2675 - loss: 1.3841\n",
      "Epoch 81: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - categorical_accuracy: 0.2710 - loss: 1.3838 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 82/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.3177 - loss: 1.3770\n",
      "Epoch 82: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - categorical_accuracy: 0.3120 - loss: 1.3780 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 83/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.2449 - loss: 1.3916\n",
      "Epoch 83: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.2525 - loss: 1.3900 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 84/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - categorical_accuracy: 0.2648 - loss: 1.3853\n",
      "Epoch 84: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.2688 - loss: 1.3847 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 85/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.2833 - loss: 1.3805\n",
      "Epoch 85: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.2839 - loss: 1.3808 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 86/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - categorical_accuracy: 0.2521 - loss: 1.3863\n",
      "Epoch 86: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - categorical_accuracy: 0.2584 - loss: 1.3856 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 87/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - categorical_accuracy: 0.3248 - loss: 1.3786\n",
      "Epoch 87: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.3179 - loss: 1.3793 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 88/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.2734 - loss: 1.3875\n",
      "Epoch 88: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 171ms/step - categorical_accuracy: 0.2746 - loss: 1.3871 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 89/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.3285 - loss: 1.3747\n",
      "Epoch 89: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - categorical_accuracy: 0.3209 - loss: 1.3761 - val_categorical_accuracy: 0.1000 - val_loss: 1.4186 - learning_rate: 1.0000e-05\n",
      "Epoch 89: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">267,264</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │ permute[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_vec (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_mul (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │                           │                 │ attention_vec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7680</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_mul[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,966,336</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,028</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m132\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m267,264\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ permute (\u001b[38;5;33mPermute\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m30\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m30\u001b[0m)           │             \u001b[38;5;34m930\u001b[0m │ permute[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_vec (\u001b[38;5;33mPermute\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_mul (\u001b[38;5;33mMultiply\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │                           │                 │ attention_vec[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7680\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ attention_mul[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │       \u001b[38;5;34m1,966,336\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 │           \u001b[38;5;34m1,028\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,558</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,235,558\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,558</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,235,558\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2523 - loss: 5.4790\n",
      "Epoch 1: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 295ms/step - categorical_accuracy: 0.2573 - loss: 5.3264 - val_categorical_accuracy: 0.1000 - val_loss: 2.8767 - learning_rate: 0.0010\n",
      "Epoch 2/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2443 - loss: 2.5222\n",
      "Epoch 2: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.2472 - loss: 2.4679 - val_categorical_accuracy: 0.1000 - val_loss: 1.6976 - learning_rate: 0.0010\n",
      "Epoch 3/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2539 - loss: 1.6061\n",
      "Epoch 3: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.2575 - loss: 1.5944 - val_categorical_accuracy: 0.1000 - val_loss: 1.4724 - learning_rate: 0.0010\n",
      "Epoch 4/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3586 - loss: 1.4396\n",
      "Epoch 4: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.3467 - loss: 1.4404 - val_categorical_accuracy: 0.1000 - val_loss: 1.4594 - learning_rate: 0.0010\n",
      "Epoch 5/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2747 - loss: 1.4367\n",
      "Epoch 5: val_loss did not improve from 1.41602\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - categorical_accuracy: 0.2805 - loss: 1.4348 - val_categorical_accuracy: 0.1000 - val_loss: 1.4265 - learning_rate: 0.0010\n",
      "Epoch 6/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2691 - loss: 1.4058\n",
      "Epoch 6: val_loss improved from 1.41602 to 1.41487, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_06-1.41.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - categorical_accuracy: 0.2674 - loss: 1.4052 - val_categorical_accuracy: 0.1000 - val_loss: 1.4149 - learning_rate: 0.0010\n",
      "Epoch 7/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2615 - loss: 1.3949\n",
      "Epoch 7: val_loss improved from 1.41487 to 1.41194, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_07-1.41.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - categorical_accuracy: 0.2648 - loss: 1.3943 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 0.0010\n",
      "Epoch 8/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3227 - loss: 1.3893\n",
      "Epoch 8: val_loss improved from 1.41194 to 1.40787, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_08-1.41.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - categorical_accuracy: 0.3210 - loss: 1.3897 - val_categorical_accuracy: 0.1000 - val_loss: 1.4079 - learning_rate: 0.0010\n",
      "Epoch 9/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.3007 - loss: 1.3873\n",
      "Epoch 9: val_loss improved from 1.40787 to 1.40402, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_09-1.40.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - categorical_accuracy: 0.2994 - loss: 1.3871 - val_categorical_accuracy: 0.1000 - val_loss: 1.4040 - learning_rate: 0.0010\n",
      "Epoch 10/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.2641 - loss: 1.3882\n",
      "Epoch 10: val_loss did not improve from 1.40402\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.2662 - loss: 1.3879 - val_categorical_accuracy: 0.1000 - val_loss: 1.4101 - learning_rate: 0.0010\n",
      "Epoch 11/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3106 - loss: 1.3831\n",
      "Epoch 11: val_loss did not improve from 1.40402\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.3063 - loss: 1.3836 - val_categorical_accuracy: 0.1000 - val_loss: 1.4152 - learning_rate: 0.0010\n",
      "Epoch 12/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3136 - loss: 1.3854\n",
      "Epoch 12: val_loss did not improve from 1.40402\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - categorical_accuracy: 0.3075 - loss: 1.3861 - val_categorical_accuracy: 0.1000 - val_loss: 1.4103 - learning_rate: 0.0010\n",
      "Epoch 13/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - categorical_accuracy: 0.2528 - loss: 1.3866\n",
      "Epoch 13: val_loss improved from 1.40402 to 1.40133, saving model to D:\\uni\\Exercise_Recognition_AI\\data\\model_13-1.40.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - categorical_accuracy: 0.2590 - loss: 1.3864 - val_categorical_accuracy: 0.1000 - val_loss: 1.4013 - learning_rate: 0.0010\n",
      "Epoch 14/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.3101 - loss: 1.3837\n",
      "Epoch 14: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - categorical_accuracy: 0.3059 - loss: 1.3841 - val_categorical_accuracy: 0.1000 - val_loss: 1.4065 - learning_rate: 0.0010\n",
      "Epoch 15/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2827 - loss: 1.3860\n",
      "Epoch 15: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - categorical_accuracy: 0.2834 - loss: 1.3857 - val_categorical_accuracy: 0.1000 - val_loss: 1.4087 - learning_rate: 0.0010\n",
      "Epoch 16/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2863 - loss: 1.3856\n",
      "Epoch 16: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - categorical_accuracy: 0.2864 - loss: 1.3853 - val_categorical_accuracy: 0.1000 - val_loss: 1.4117 - learning_rate: 0.0010\n",
      "Epoch 17/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.2071 - loss: 1.3970\n",
      "Epoch 17: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.2137 - loss: 1.3961 - val_categorical_accuracy: 0.1000 - val_loss: 1.4036 - learning_rate: 0.0010\n",
      "Epoch 18/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2859 - loss: 1.3846\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.2860 - loss: 1.3849 - val_categorical_accuracy: 0.1000 - val_loss: 1.4064 - learning_rate: 0.0010\n",
      "Epoch 19/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2661 - loss: 1.3869\n",
      "Epoch 19: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - categorical_accuracy: 0.2698 - loss: 1.3863 - val_categorical_accuracy: 0.1000 - val_loss: 1.4073 - learning_rate: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2859 - loss: 1.3860\n",
      "Epoch 20: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - categorical_accuracy: 0.2860 - loss: 1.3859 - val_categorical_accuracy: 0.1000 - val_loss: 1.4081 - learning_rate: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3162 - loss: 1.3790\n",
      "Epoch 21: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.3108 - loss: 1.3799 - val_categorical_accuracy: 0.1000 - val_loss: 1.4124 - learning_rate: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.3007 - loss: 1.3808\n",
      "Epoch 22: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - categorical_accuracy: 0.2981 - loss: 1.3814 - val_categorical_accuracy: 0.1000 - val_loss: 1.4116 - learning_rate: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3232 - loss: 1.3793\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.3166 - loss: 1.3800 - val_categorical_accuracy: 0.1000 - val_loss: 1.4111 - learning_rate: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3371 - loss: 1.3765\n",
      "Epoch 24: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - categorical_accuracy: 0.3280 - loss: 1.3778 - val_categorical_accuracy: 0.1000 - val_loss: 1.4121 - learning_rate: 2.5000e-04\n",
      "Epoch 25/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2849 - loss: 1.3862\n",
      "Epoch 25: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - categorical_accuracy: 0.2852 - loss: 1.3862 - val_categorical_accuracy: 0.1000 - val_loss: 1.4101 - learning_rate: 2.5000e-04\n",
      "Epoch 26/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2851 - loss: 1.3816\n",
      "Epoch 26: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.2854 - loss: 1.3819 - val_categorical_accuracy: 0.1000 - val_loss: 1.4099 - learning_rate: 2.5000e-04\n",
      "Epoch 27/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.2775 - loss: 1.3848\n",
      "Epoch 27: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - categorical_accuracy: 0.2784 - loss: 1.3847 - val_categorical_accuracy: 0.1000 - val_loss: 1.4093 - learning_rate: 2.5000e-04\n",
      "Epoch 28/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2774 - loss: 1.3850\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - categorical_accuracy: 0.2791 - loss: 1.3845 - val_categorical_accuracy: 0.1000 - val_loss: 1.4092 - learning_rate: 2.5000e-04\n",
      "Epoch 29/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - categorical_accuracy: 0.2898 - loss: 1.3826\n",
      "Epoch 29: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.2892 - loss: 1.3827 - val_categorical_accuracy: 0.1000 - val_loss: 1.4096 - learning_rate: 1.2500e-04\n",
      "Epoch 30/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3109 - loss: 1.3785\n",
      "Epoch 30: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.3065 - loss: 1.3795 - val_categorical_accuracy: 0.1000 - val_loss: 1.4104 - learning_rate: 1.2500e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2607 - loss: 1.3917\n",
      "Epoch 31: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - categorical_accuracy: 0.2654 - loss: 1.3905 - val_categorical_accuracy: 0.1000 - val_loss: 1.4098 - learning_rate: 1.2500e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2497 - loss: 1.3823\n",
      "Epoch 32: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - categorical_accuracy: 0.2564 - loss: 1.3820 - val_categorical_accuracy: 0.1000 - val_loss: 1.4104 - learning_rate: 1.2500e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.3082 - loss: 1.3800\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - categorical_accuracy: 0.3043 - loss: 1.3803 - val_categorical_accuracy: 0.1000 - val_loss: 1.4117 - learning_rate: 1.2500e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2489 - loss: 1.3902\n",
      "Epoch 34: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - categorical_accuracy: 0.2558 - loss: 1.3891 - val_categorical_accuracy: 0.1000 - val_loss: 1.4118 - learning_rate: 6.2500e-05\n",
      "Epoch 35/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - categorical_accuracy: 0.2386 - loss: 1.3894\n",
      "Epoch 35: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.2430 - loss: 1.3889 - val_categorical_accuracy: 0.1000 - val_loss: 1.4115 - learning_rate: 6.2500e-05\n",
      "Epoch 36/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3167 - loss: 1.3788\n",
      "Epoch 36: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - categorical_accuracy: 0.3112 - loss: 1.3796 - val_categorical_accuracy: 0.1000 - val_loss: 1.4113 - learning_rate: 6.2500e-05\n",
      "Epoch 37/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2595 - loss: 1.3859\n",
      "Epoch 37: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - categorical_accuracy: 0.2644 - loss: 1.3848 - val_categorical_accuracy: 0.1000 - val_loss: 1.4113 - learning_rate: 6.2500e-05\n",
      "Epoch 38/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2496 - loss: 1.3839\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.2563 - loss: 1.3836 - val_categorical_accuracy: 0.1000 - val_loss: 1.4116 - learning_rate: 6.2500e-05\n",
      "Epoch 39/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3009 - loss: 1.3771\n",
      "Epoch 39: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.2983 - loss: 1.3779 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 3.1250e-05\n",
      "Epoch 40/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3465 - loss: 1.3717\n",
      "Epoch 40: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.3357 - loss: 1.3735 - val_categorical_accuracy: 0.1000 - val_loss: 1.4122 - learning_rate: 3.1250e-05\n",
      "Epoch 41/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2788 - loss: 1.3826\n",
      "Epoch 41: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - categorical_accuracy: 0.2803 - loss: 1.3827 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 3.1250e-05\n",
      "Epoch 42/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2768 - loss: 1.3872\n",
      "Epoch 42: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.2786 - loss: 1.3867 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 3.1250e-05\n",
      "Epoch 43/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3346 - loss: 1.3761\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.3259 - loss: 1.3773 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 3.1250e-05\n",
      "Epoch 44/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2650 - loss: 1.3841\n",
      "Epoch 44: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.2689 - loss: 1.3840 - val_categorical_accuracy: 0.1000 - val_loss: 1.4118 - learning_rate: 1.5625e-05\n",
      "Epoch 45/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3339 - loss: 1.3790\n",
      "Epoch 45: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.3253 - loss: 1.3797 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 1.5625e-05\n",
      "Epoch 46/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2656 - loss: 1.3866\n",
      "Epoch 46: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - categorical_accuracy: 0.2695 - loss: 1.3859 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 1.5625e-05\n",
      "Epoch 47/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2669 - loss: 1.3871\n",
      "Epoch 47: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.2705 - loss: 1.3864 - val_categorical_accuracy: 0.1000 - val_loss: 1.4118 - learning_rate: 1.5625e-05\n",
      "Epoch 48/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.2538 - loss: 1.3907\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.2597 - loss: 1.3896 - val_categorical_accuracy: 0.1000 - val_loss: 1.4118 - learning_rate: 1.5625e-05\n",
      "Epoch 49/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.2937 - loss: 1.3841\n",
      "Epoch 49: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.2924 - loss: 1.3840 - val_categorical_accuracy: 0.1000 - val_loss: 1.4118 - learning_rate: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3323 - loss: 1.3759\n",
      "Epoch 50: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.3240 - loss: 1.3770 - val_categorical_accuracy: 0.1000 - val_loss: 1.4119 - learning_rate: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - categorical_accuracy: 0.2869 - loss: 1.3855\n",
      "Epoch 51: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.2869 - loss: 1.3855 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2603 - loss: 1.3859\n",
      "Epoch 52: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.2651 - loss: 1.3851 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 53/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2990 - loss: 1.3812\n",
      "Epoch 53: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.2968 - loss: 1.3820 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 54/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2656 - loss: 1.3861\n",
      "Epoch 54: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.2694 - loss: 1.3854 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - categorical_accuracy: 0.2222 - loss: 1.3923\n",
      "Epoch 55: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - categorical_accuracy: 0.2281 - loss: 1.3917 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 56/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2402 - loss: 1.3900\n",
      "Epoch 56: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.2487 - loss: 1.3888 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 57/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.2828 - loss: 1.3820\n",
      "Epoch 57: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.2835 - loss: 1.3825 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.2889 - loss: 1.3785\n",
      "Epoch 58: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - categorical_accuracy: 0.2887 - loss: 1.3788 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 59/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3215 - loss: 1.3755\n",
      "Epoch 59: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.3151 - loss: 1.3764 - val_categorical_accuracy: 0.1000 - val_loss: 1.4121 - learning_rate: 1.0000e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - categorical_accuracy: 0.3503 - loss: 1.3715\n",
      "Epoch 60: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - categorical_accuracy: 0.3387 - loss: 1.3735 - val_categorical_accuracy: 0.1000 - val_loss: 1.4122 - learning_rate: 1.0000e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - categorical_accuracy: 0.2847 - loss: 1.3838\n",
      "Epoch 61: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - categorical_accuracy: 0.2849 - loss: 1.3839 - val_categorical_accuracy: 0.1000 - val_loss: 1.4121 - learning_rate: 1.0000e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.2352 - loss: 1.3947\n",
      "Epoch 62: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - categorical_accuracy: 0.2445 - loss: 1.3926 - val_categorical_accuracy: 0.1000 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - categorical_accuracy: 0.3072 - loss: 1.3801\n",
      "Epoch 63: val_loss did not improve from 1.40133\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - categorical_accuracy: 0.3035 - loss: 1.3809 - val_categorical_accuracy: 0.1000 - val_loss: 1.4121 - learning_rate: 1.0000e-05\n",
      "Epoch 63: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Callbacks to be used during neural network training \n",
    "es_callback = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=50, verbose=1, mode='min')  # Aumentei a paciência para 50\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1, mode='min')\n",
    "chkpt_callback = ModelCheckpoint(filepath=os.path.join(DATA_PATH, 'model_{epoch:02d}-{val_loss:.2f}.keras'), \n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq='epoch')\n",
    "\n",
    "# some hyperparameters\n",
    "batch_size = 16\n",
    "max_epochs = 1000\n",
    "\n",
    "## 6a. LSTM\n",
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-LSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME, '')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]\n",
    "\n",
    "# Optimizer para o modelo LSTM\n",
    "opt_lstm = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Reestruturar o modelo para usar uma camada Input explícita\n",
    "lstm = Sequential([\n",
    "    Input(shape=(sequence_length, 132)),  # 33 landmarks * 4 valores = 132\n",
    "    LSTM(128, return_sequences=True, activation='relu'),\n",
    "    LSTM(256, return_sequences=True, activation='relu'),\n",
    "    LSTM(128, return_sequences=False, activation='relu'),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Adicionei regularização L2\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),   # Adicionei regularização L2\n",
    "    Dense(actions.shape[0], activation='softmax')\n",
    "])\n",
    "print(lstm.summary())\n",
    "\n",
    "lstm.compile(optimizer=opt_lstm, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "\n",
    "## 6b. LSTM + Attention\n",
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME, '')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]\n",
    "\n",
    "# Optimizer para o modelo AttnLSTM\n",
    "opt_attn = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul\n",
    "\n",
    "HIDDEN_UNITS = 128\n",
    "DROPOUT_RATE = 0.5  # Aumentei a taxa de dropout para 0.5 para reduzir overfitting\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(sequence_length, 132))  # 33 landmarks * 4 valores = 132\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "\n",
    "# Attention\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(attention_mul)  # Adicionei regularização L2\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "# Output\n",
    "x = Dense(actions.shape[0], activation='softmax')(x)\n",
    "\n",
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())\n",
    "\n",
    "AttnLSTM.compile(optimizer=opt_attn, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "\n",
    "# Model map\n",
    "models = {\n",
    "    'LSTM': lstm, \n",
    "    'LSTM_Attention_128HUs': AttnLSTM, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f612",
   "metadata": {},
   "source": [
    "# 7a. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7647ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fecf26",
   "metadata": {},
   "source": [
    "# 7b. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0114a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model rebuild before doing this\n",
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7747c6",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2101a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    res = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c98e",
   "metadata": {},
   "source": [
    "# 9. Evaluations using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf242d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM confusion matrix: \n",
      "[[[13  0]\n",
      "  [ 7  0]]\n",
      "\n",
      " [[16  0]\n",
      "  [ 4  0]]\n",
      "\n",
      " [[ 0 16]\n",
      "  [ 0  4]]\n",
      "\n",
      " [[15  0]\n",
      "  [ 5  0]]]\n",
      "LSTM_Attention_128HUs confusion matrix: \n",
      "[[[13  0]\n",
      "  [ 7  0]]\n",
      "\n",
      " [[16  0]\n",
      "  [ 4  0]]\n",
      "\n",
      " [[ 0 16]\n",
      "  [ 0  4]]\n",
      "\n",
      " [[15  0]\n",
      "  [ 5  0]]]\n",
      "LSTM classification accuracy = 20.0%\n",
      "LSTM_Attention_128HUs classification accuracy = 20.0%\n",
      "LSTM weighted average precision = 0.04\n",
      "LSTM weighted average recall = 0.2\n",
      "LSTM weighted average f1-score = 0.067\\n\n",
      "LSTM_Attention_128HUs weighted average precision = 0.04\n",
      "LSTM_Attention_128HUs weighted average recall = 0.2\n",
      "LSTM_Attention_128HUs weighted average f1-score = 0.067\\n\n"
     ]
    }
   ],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} \n",
    "\n",
    "## 9a. Confusion Matrices\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['confusion matrix'] = confusion_matrices\n",
    "\n",
    "## 9b. Accuracy\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Model accuracy\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['accuracy'] = classification_accuracies\n",
    "\n",
    "## 9c. Precision, Recall, and F1 Score\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Precision, recall, and f1 score\n",
    "    # Adicionei zero_division=0 para suprimir o aviso\n",
    "    report = classification_report(ytrue, yhat, target_names=actions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "   \n",
    "    print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\\\n\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39476",
   "metadata": {},
   "source": [
    "# 10. Choose Model to Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0015ce",
   "metadata": {},
   "source": [
    "# 11. Calculate Joint Angles & Count Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f172932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Computes 3D joint angle inferred by 3 keypoints and their relative positions to one another\n",
    "    \n",
    "    \"\"\"\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle \n",
    "\n",
    "def get_coordinates(landmarks, mp_pose, side, joint):\n",
    "    \"\"\"\n",
    "    Retrieves x and y coordinates of a particular keypoint from the pose estimation model\n",
    "         \n",
    "     Args:\n",
    "         landmarks: processed keypoints from the pose estimation model\n",
    "         mp_pose: Mediapipe pose estimation model\n",
    "         side: 'left' or 'right'. Denotes the side of the body of the landmark of interest.\n",
    "         joint: 'shoulder', 'elbow', 'wrist', 'hip', 'knee', or 'ankle'. Denotes which body joint is associated with the landmark of interest.\n",
    "    \n",
    "    \"\"\"\n",
    "    coord = getattr(mp_pose.PoseLandmark, side.upper() + \"_\" + joint.upper())\n",
    "    x_coord_val = landmarks[coord.value].x\n",
    "    y_coord_val = landmarks[coord.value].y\n",
    "    return [x_coord_val, y_coord_val]            \n",
    "\n",
    "def viz_joint_angle(image, angle, joint):\n",
    "    \"\"\"\n",
    "    Displays the joint angle value near the joint within the image frame\n",
    "    \n",
    "    \"\"\"\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return\n",
    "\n",
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Counts repetitions of each exercise. Global count and stage (i.e., state) variables are updated within this function.\n",
    "    Returns a dictionary with additional info for visualization, including quality feedback.\n",
    "    \n",
    "    \"\"\"\n",
    "    global curl_counter, press_counter, squat_counter, pushup_counter, curl_stage, press_stage, squat_stage, pushup_stage\n",
    "    \n",
    "    # Inicializar contadores e estados se não existirem\n",
    "    if 'pushup_counter' not in globals():\n",
    "        globals()['pushup_counter'] = 0\n",
    "    if 'pushup_stage' not in globals():\n",
    "        globals()['pushup_stage'] = None\n",
    "    \n",
    "    # Dicionário para armazenar informações de depuração e feedback\n",
    "    debug_info = {\n",
    "        \"squat_stage\": squat_stage,\n",
    "        \"pushup_stage\": pushup_stage,\n",
    "        \"left_knee_angle\": None,\n",
    "        \"right_knee_angle\": None,\n",
    "        \"left_hip_angle\": None,\n",
    "        \"right_hip_angle\": None,\n",
    "        \"left_elbow_angle\": None,  # Para flexões\n",
    "        \"right_elbow_angle\": None,  # Para flexões\n",
    "        \"message\": \"\",\n",
    "        \"quality_feedback\": \"\",  # Feedback sobre a qualidade do movimento\n",
    "        \"squat_status\": \"\"  # Novo campo para \"Squat bem realizado!\" ou \"Squat realizado de forma incorreta!\"\n",
    "    }\n",
    "    \n",
    "    if current_action == 'curl':\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        \n",
    "        angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        if angle < 30:\n",
    "            curl_stage = \"up\" \n",
    "        if angle > 140 and curl_stage == 'up':\n",
    "            curl_stage = \"down\"  \n",
    "            curl_counter += 1\n",
    "            debug_info[\"message\"] = f\"Curl detetado! Contador: {curl_counter}\"\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        pushup_stage = None\n",
    "            \n",
    "        viz_joint_angle(image, angle, elbow)\n",
    "        \n",
    "    elif current_action == 'press':\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "        elbow_angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        shoulder2elbow_dist = abs(math.dist(shoulder, elbow))\n",
    "        shoulder2wrist_dist = abs(math.dist(shoulder, wrist))\n",
    "        \n",
    "        if (elbow_angle > 130) and (shoulder2elbow_dist < shoulder2wrist_dist):\n",
    "            press_stage = \"up\"\n",
    "        if (elbow_angle < 50) and (shoulder2elbow_dist > shoulder2wrist_dist) and (press_stage == 'up'):\n",
    "            press_stage = 'down'\n",
    "            press_counter += 1\n",
    "            debug_info[\"message\"] = f\"Press detetado! Contador: {press_counter}\"\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "        pushup_stage = None\n",
    "            \n",
    "        viz_joint_angle(image, elbow_angle, elbow)\n",
    "        \n",
    "    elif current_action == 'squat':\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        right_shoulder = get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_hip = get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "        \n",
    "        # Guardar ângulos no dicionário para visualização\n",
    "        debug_info[\"left_knee_angle\"] = left_knee_angle\n",
    "        debug_info[\"right_knee_angle\"] = right_knee_angle\n",
    "        debug_info[\"left_hip_angle\"] = left_hip_angle\n",
    "        debug_info[\"right_hip_angle\"] = right_hip_angle\n",
    "        \n",
    "        # Adicionar mensagens de depuração para os ângulos\n",
    "        print(f\"Left Knee Angle: {left_knee_angle:.2f}, Right Knee Angle: {right_knee_angle:.2f}, \"\n",
    "              f\"Left Hip Angle: {left_hip_angle:.2f}, Right Hip Angle: {right_hip_angle:.2f}\")\n",
    "        \n",
    "        # Critérios para um \"bom agachamento\"\n",
    "        depth_threshold = 90  # Ângulo dos joelhos para profundidade suficiente\n",
    "        alignment_threshold = 10  # Diferença máxima entre ângulos dos quadris para alinhamento\n",
    "        thr = 100  # Reduzi o limiar para 100 para facilitar a deteção\n",
    "        \n",
    "        # Verificar a profundidade e o alinhamento\n",
    "        is_depth_good = left_knee_angle < depth_threshold and right_knee_angle < depth_threshold\n",
    "        is_alignment_good = abs(left_hip_angle - right_hip_angle) <= alignment_threshold\n",
    "        \n",
    "        if is_depth_good and is_alignment_good:\n",
    "            debug_info[\"quality_feedback\"] = \"Agachamento Correto!\"\n",
    "        else:\n",
    "            debug_info[\"quality_feedback\"] = \"\"\n",
    "            if left_knee_angle > depth_threshold or right_knee_angle > depth_threshold:\n",
    "                debug_info[\"quality_feedback\"] += \"Desça Mais! \"\n",
    "            if abs(left_hip_angle - right_hip_angle) > alignment_threshold:\n",
    "                debug_info[\"quality_feedback\"] += \"Alinhe os Quadris!\"\n",
    "        \n",
    "        # Lógica de contagem de repetições\n",
    "        if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "            squat_stage = \"down\"\n",
    "            debug_info[\"squat_stage\"] = squat_stage\n",
    "        if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (squat_stage == 'down'):\n",
    "            squat_stage = 'up'\n",
    "            squat_counter += 1\n",
    "            debug_info[\"squat_stage\"] = squat_stage\n",
    "            debug_info[\"message\"] = f\"Agachamento detetado! Contador: {squat_counter}\"\n",
    "            # Adicionar mensagem de status do squat\n",
    "            if is_depth_good and is_alignment_good:\n",
    "                debug_info[\"squat_status\"] = \"Squat bem realizado!\"\n",
    "            else:\n",
    "                debug_info[\"squat_status\"] = \"Squat realizado de forma incorreta!\"\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "        pushup_stage = None\n",
    "            \n",
    "        viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "        viz_joint_angle(image, left_hip_angle, left_hip)\n",
    "        \n",
    "    elif current_action == 'pushup':\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        left_wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        right_shoulder = get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_elbow = get_coordinates(landmarks, mp_pose, 'right', 'elbow')\n",
    "        right_wrist = get_coordinates(landmarks, mp_pose, 'right', 'wrist')\n",
    "        \n",
    "        left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "        \n",
    "        debug_info[\"left_elbow_angle\"] = left_elbow_angle\n",
    "        debug_info[\"right_elbow_angle\"] = right_elbow_angle\n",
    "        \n",
    "        # Critérios para uma \"boa flexão\"\n",
    "        pushup_depth_threshold = 90  # Ângulo dos cotovelos para profundidade suficiente\n",
    "        if left_elbow_angle < pushup_depth_threshold and right_elbow_angle < pushup_depth_threshold:\n",
    "            debug_info[\"quality_feedback\"] = \"Flexão Correta!\"\n",
    "        elif left_elbow_angle > pushup_depth_threshold or right_elbow_angle > pushup_depth_threshold:\n",
    "            debug_info[\"quality_feedback\"] = \"Desça Mais!\"\n",
    "        \n",
    "        # Lógica de contagem de repetições para flexões\n",
    "        pushup_thr = 120\n",
    "        if (left_elbow_angle < pushup_thr) and (right_elbow_angle < pushup_thr):\n",
    "            pushup_stage = \"down\"\n",
    "            debug_info[\"pushup_stage\"] = pushup_stage\n",
    "        if (left_elbow_angle > pushup_thr) and (right_elbow_angle > pushup_thr) and (pushup_stage == 'down'):\n",
    "            pushup_stage = 'up'\n",
    "            pushup_counter += 1\n",
    "            debug_info[\"pushup_stage\"] = pushup_stage\n",
    "            debug_info[\"message\"] = f\"Flexão detetada! Contador: {pushup_counter}\"\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        \n",
    "        viz_joint_angle(image, left_elbow_angle, left_elbow)\n",
    "        viz_joint_angle(image, right_elbow_angle, right_elbow)\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5116ef6",
   "metadata": {},
   "source": [
    "# 12. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4775b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processamento do vídeo em tempo real...\n"
     ]
    }
   ],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    \"\"\"\n",
    "    This function displays the model prediction probability distribution over the set of exercise classes\n",
    "    as a horizontal bar graph\n",
    "    \n",
    "    \"\"\"\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 3, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "predictions = []\n",
    "# Inicializar res com zeros para evitar problemas na primeira iteração\n",
    "res = np.zeros(len(actions))  # Array de zeros com o tamanho do número de classes\n",
    "threshold = 0.5 # minimum confidence to classify as an action/exercise\n",
    "current_action = ''\n",
    "confidence = 0.0  # Inicializar confidence com 0.0\n",
    "\n",
    "# Rep counter logic variables\n",
    "curl_counter = 0\n",
    "press_counter = 0\n",
    "squat_counter = 0\n",
    "pushup_counter = 0\n",
    "curl_stage = None\n",
    "press_stage = None\n",
    "squat_stage = None\n",
    "pushup_stage = None\n",
    "\n",
    "# Contador para controlar a frequência das predições\n",
    "prediction_counter = 0\n",
    "PREDICTION_INTERVAL = 5  # Fazer predição a cada 5 frames\n",
    "\n",
    "# Dimensões da janela de visualização\n",
    "DISPLAY_WIDTH = 960\n",
    "DISPLAY_HEIGHT = 540\n",
    "\n",
    "# Camera object\n",
    "cap = cv2.VideoCapture('D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir o arquivo de vídeo D:\\\\uni\\\\Exercise_Recognition_AI\\\\videos\\\\exercicio.mp4\")\n",
    "    exit()\n",
    "\n",
    "# Video writer object that saves a video of the real time test\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') # video compression format\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # webcam video frame height\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # webcam video frame width\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS)) # webcam video frame rate \n",
    "\n",
    "video_name = os.path.join(os.getcwd(), f\"{model_name}_real_time_test.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, \n",
    "                  static_image_mode=False, model_complexity=1) as pose:\n",
    "    print(\"Iniciando o processamento do vídeo em tempo real...\")\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fim do vídeo, reiniciando...\")\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia o vídeo\n",
    "            continue\n",
    "\n",
    "        # Redimensionar o frame para fornecer dimensões consistentes ao MediaPipe\n",
    "        image = cv2.resize(frame, (WIDTH, HEIGHT))\n",
    "        \n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(image, pose)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        prediction_counter += 1\n",
    "        if len(sequence) == sequence_length and prediction_counter % PREDICTION_INTERVAL == 0:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(res))\n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "            # Erase current action variable if no probability is above threshold\n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "        \n",
    "        # 3. Viz logic\n",
    "        # Viz probabilities\n",
    "        image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        # Count reps and get debug info\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            debug_info = count_reps(image, current_action, landmarks, mp_pose)\n",
    "        except:\n",
    "            debug_info = {\n",
    "                \"squat_stage\": None,\n",
    "                \"pushup_stage\": None,\n",
    "                \"left_knee_angle\": None,\n",
    "                \"right_knee_angle\": None,\n",
    "                \"left_hip_angle\": None,\n",
    "                \"right_hip_angle\": None,\n",
    "                \"left_elbow_angle\": None,\n",
    "                \"right_elbow_angle\": None,\n",
    "                \"message\": \"\",\n",
    "                \"quality_feedback\": \"\",\n",
    "                \"squat_status\": \"\"\n",
    "            }\n",
    "\n",
    "        # Redimensionar o frame para visualização (tamanho fixo para evitar zoom descomunal)\n",
    "        display_image = cv2.resize(image, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "\n",
    "        # Ajustar as posições e tamanhos dos retângulos e textos para as dimensões de visualização\n",
    "        # Fundo para a barra superior\n",
    "        cv2.rectangle(display_image, (0, 0), (DISPLAY_WIDTH, 40), colors[np.argmax(res)], -1)\n",
    "        \n",
    "        # Exibir a ação detetada com fundo opaco\n",
    "        text = f'Action: {current_action}'\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "        cv2.rectangle(display_image, (10, 10), (10 + text_width, 10 + text_height + 10), (0, 0, 0), -1)\n",
    "        cv2.putText(display_image, text, (10, 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Contadores de repetições com fundo opaco\n",
    "        y_offset = 90\n",
    "        for label, count in [(\"Curl\", curl_counter), (\"Press\", press_counter), (\"Squat\", squat_counter), (\"Pushup\", pushup_counter)]:\n",
    "            text = f'{label}: {count}'\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "            cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "            cv2.putText(display_image, text, (10, y_offset), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 3, cv2.LINE_AA)\n",
    "            y_offset += 60\n",
    "        \n",
    "        # Exibir informações de agachamento com fundo opaco\n",
    "        if current_action == 'squat':\n",
    "            y_offset = 210\n",
    "            text = f'Squat Stage: {debug_info[\"squat_stage\"]}'\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "            cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "            cv2.putText(display_image, text, (10, y_offset), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "            y_offset += 60\n",
    "            \n",
    "            if debug_info[\"quality_feedback\"]:\n",
    "                text = debug_info[\"quality_feedback\"]\n",
    "                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "                cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "                cv2.putText(display_image, text, (10, y_offset), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 3, cv2.LINE_AA)\n",
    "                y_offset += 60\n",
    "            \n",
    "            if debug_info[\"squat_status\"]:\n",
    "                text = debug_info[\"squat_status\"]\n",
    "                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "                cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "                cv2.putText(display_image, text, (10, y_offset), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "                y_offset += 60\n",
    "            \n",
    "            if debug_info[\"message\"]:\n",
    "                text = debug_info[\"message\"]\n",
    "                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "                cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "                cv2.putText(display_image, text, (10, y_offset), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Exibir informações de flexões com fundo opaco\n",
    "        elif current_action == 'pushup':\n",
    "            y_offset = 210\n",
    "            text = f'Pushup Stage: {debug_info[\"pushup_stage\"]}'\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "            cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "            cv2.putText(display_image, text, (10, y_offset), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "            y_offset += 60\n",
    "            \n",
    "            if debug_info[\"quality_feedback\"]:\n",
    "                text = debug_info[\"quality_feedback\"]\n",
    "                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "                cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "                cv2.putText(display_image, text, (10, y_offset), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 3, cv2.LINE_AA)\n",
    "                y_offset += 60\n",
    "            \n",
    "            if debug_info[\"message\"]:\n",
    "                text = debug_info[\"message\"]\n",
    "                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)\n",
    "                cv2.rectangle(display_image, (10, y_offset - text_height - 5), (10 + text_width, y_offset + 5), (0, 0, 0), -1)\n",
    "                cv2.putText(display_image, text, (10, y_offset), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "     \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', display_image)\n",
    "        cv2.waitKey(1)  # Mantive 1ms para acelerar o frame rate\n",
    "        \n",
    "        # Write to video file\n",
    "        if ret:\n",
    "            out.write(image)  # Salvar o frame nas dimensões originais (WIDTH, HEIGHT)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
